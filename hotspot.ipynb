{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on Traffic Hotspot\n",
    "In the following program, we would guide you through use Tensorflow library to load and process traffic data. Then we would teach you how to create and train your Tensorflow model.\n",
    "\n",
    "**Note: Hit the \"Run\" button to run the program block by block. We don't recommend you to use \"Run All\" in \"Cell\" because the first few blocks only need to be run once and they take some time to run.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "The following block is used in Python to import necessary libraries. You might encounter error while trying to import tensorflow. This is becuase Tensorflow is not a default library that comes with the Python package you installed. Go to this link https://www.tensorflow.org/install/pip#system-install and follow the instructions on installing Tensorflow. If you encounter problems while trying to install Tensorflow you can add `--user` after `pip install`. This is because you did not create a virtual environment for your python packages. You can follow Step 2 on the website to create a virtual environment (recommended) or you can just install the package in your HOME environment.\n",
    "\n",
    "* `pandas` is used to process our data.\n",
    "\n",
    "* `numpy` is a great tool for mathematical processing and array creations.\n",
    "\n",
    "* `sklearn` is used to split the data into Training, Testing, and Validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean up the Dataset\n",
    "To process the data, save the data.txt and label.txt file you downloaded from the Google Drive to the same directory where this Notebook is at.\n",
    "* `with open(label_src, 'r')` reads the data into label\n",
    "\n",
    "    * Note that we call `np` directly becuase we import `numpy as np`\n",
    "    \n",
    "* `.from_tensor_slices()` .would bind numpy array data and label together to creat a tensorflow data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_src = \"data.txt\"\n",
    "label_src = \"label.txt\"\n",
    "\n",
    "with open(label_src, 'r') as f:\n",
    "    label = np.array(f.read().split())\n",
    "label = label.astype(\"float\")\n",
    "\n",
    "data = []\n",
    "with open(data_src, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(line.split())\n",
    "data = np.array(data)\n",
    "data = data.astype(\"float\")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seperated the block above from the block below becuase we don't want you to read the data twice. Reading a large file as you might have experienced a few minutes ago take up quite some RAM and CPU.\n",
    "\n",
    "**To-do:** \n",
    "1. Print out the **shape** of our datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"shape of features is {}\".format(np.shape(data)))\n",
    "print(\"shape of labels is {}\".format(np.shape(label)))\n",
    "### Insert your code below ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, you acquire the shape of our dataset, the first demension give us the number of data points, and the second dimension give us teh size of each data point.\n",
    "    For example, feature size of (4000, 800) would represent a dataset of 4000 data, and each data is a 800 entry list. When designing model later, we must make sure the the input layer have the same input size as our data size.Later,we need to shuffle the data and separate it in to batches before we can use it. \n",
    "\n",
    "\n",
    "**To-do:**\n",
    "1. Shuffle the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "SHUFFLE_BUFFER_SIZE = 100000 \n",
    "#dataset is fully shuffled only when shuffle_buffer_size larger the data size.\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset_shuffled = dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset would be divided in to different batches, we can check the number of batch by using following code. And we expect # batch = # data / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of batches in dataset is {}\".format(tf.data.experimental.cardinality(dataset_shuffled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Stop*\n",
    "*Before you proceed, make sure you finish reading \"Machine Learning Introduction\" in Step 3 of the lab. You should complete the Tensorflow playground exercise and take a screenshot of your results.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data for Machine Learning\n",
    "In machine learning, we often want to split our data into Training Set, Validation Set, and Test Set.\n",
    "* **Training Set**:\n",
    "Training Set is used to train our machine learning model while the Validation and Test Set aren't. \n",
    "* **Validation Set**:\n",
    "Having a Validation Set prevents overfitting of our machine learning model. Overfitting is when our model is tuned perfectly for a specific set of data, but is fitted poorly for other set of data. Take traffic emission data for example. If the data predicts $CO_2$ emission data within 10 mse (mean squared error) from Training Set, but predicts emission data over 50 mse from Validation data. Then we could see that the model is overfitted.\n",
    "* **Test Set**:\n",
    "Test set is used to evaluate the final model.\n",
    "\n",
    "A typical workflow will be: \n",
    "1. Train your model using *Training Set*.\n",
    "2. Validate your model using *Validation Set*.\n",
    "3. Adjust your model using results from *Validation Set*.\n",
    "4. Pick the model that produces best results from using *Validation Set*.\n",
    "5. Confirm your model with *Test Set*.\n",
    "\n",
    "**To-Do:**\n",
    "1. Tweak the `test_size` values for spilitting `train_dataset`, `test_dataset`, and `val_dataset`.\n",
    "\n",
    "2. In given code 80% of all data would be used for train, 10% data would be used for validation and 10% of data would be used for test, you can change this ratio by change parameter in calculating train_size, val_size and test_size\n",
    "\n",
    "3. You will come back and change some codes after you finish your first training. Instructions will be provided in the \"Train the Model\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = tf.data.experimental.cardinality(dataset_shuffled).numpy()\n",
    "train_size = int(0.8 * DATASET_SIZE)\n",
    "val_size = int(0.1 * DATASET_SIZE)\n",
    "test_size = int(0.1 * DATASET_SIZE)\n",
    "\n",
    "train_dataset = dataset_shuffled.take(train_size)\n",
    "test_dataset = dataset_shuffled.skip(train_size)\n",
    "val_dataset = test_dataset.skip(val_size)\n",
    "test_dataset = test_dataset.take(test_size)\n",
    "\n",
    "print(\"total dataset size {}\".format(DATASET_SIZE))\n",
    "print(\"train dataset size {}\".format(tf.data.experimental.cardinality(train_dataset)))\n",
    "print(\"test dataset size {}\".format(tf.data.experimental.cardinality(test_dataset)))\n",
    "print(\"validation dataset size {}\".format(tf.data.experimental.cardinality(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Training Function\n",
    "\n",
    "The below code block defines function for creating and trining a sequential Tensorflow model. You can also train the model without function definition. Just replace the function call in the next block after this one with: \n",
    "\n",
    "`model = tf.keras.models.Sequential()`\n",
    "<br>&emsp;&emsp;&emsp;&emsp;.\n",
    "<br>&emsp;&emsp;&emsp;&emsp;.\n",
    "<br>&emsp;&emsp;&emsp;&emsp;.\n",
    "<br>`model.save('my_model')`\n",
    "\n",
    "Also replace the input variables with the correct names. Note that the first function returns a variable. \n",
    "\n",
    "However, these function definitions make the code less messy. There is another way to write Tensorflow, which is called Functional API. It is more advanced. If you want to learn more, please go to https://www.tensorflow.org/guide/keras/functional. \n",
    "\n",
    "### Function Definition\n",
    "1. Create model \n",
    "    * `model.add()`: add layer to model\n",
    "    \n",
    "    * In `tf.keras.layers.Dense()`\n",
    "    \n",
    "        * `units`: number of nodes in that layer\n",
    "        \n",
    "        * `activation`: activation function used in that layer\n",
    "        \n",
    "        * `kernel_regularizer`: regularization function used in that layer\n",
    "        \n",
    "        * `name`: is just for us to keep track and debug\n",
    "        \n",
    "    * In `model.compile()`\n",
    "    \n",
    "        * `optimizer=tf.keras.optimizers.Adam(lr=learning_rate)`: Used to improve performance of the training\n",
    "        \n",
    "        * `Adam`: stochastic gradient descent method\n",
    "        \n",
    "        * `loss`: update the model according to specified loss function \n",
    "        \n",
    "        * `metrics`: evaluate the model according specified metrics\n",
    "        \n",
    "\n",
    "    \n",
    "**To-do:**\n",
    "See next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelCreate(learning_rate):\n",
    "    \n",
    "    # Sequential is used in most simple keras models\n",
    "    model = tf.keras.models.Sequential([\n",
    "        \n",
    "        # First hidden layer with 80 nodes\n",
    "        tf.keras.layers.Dense(units=80, \n",
    "                              activation='relu',\n",
    "                              name='Hidden1'),\n",
    "        \n",
    "        # Second hidden layer\n",
    "        tf.keras.layers.Dense(units=20, \n",
    "                              activation='softplus', \n",
    "                              name='Hidden2'),\n",
    "        \n",
    "        # Output layer\n",
    "        tf.keras.layers.Dense(units=2,  \n",
    "                              activation='softplus',\n",
    "                              name='Output')\n",
    "      \n",
    "        #Create your own model\n",
    "        \n",
    "        \n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "As we mentioned in the lab document, Hyperparameters affect the performance of your model. In the following block, you would be training your model. We also want you to experience training both a small dataset and a large dataset. \n",
    "\n",
    "**To-do:**\n",
    "    \n",
    "    1. Adjust the Hyperparameters. Remember, a large learning rate might cause the model to never converge, but a very small learning rate would cause the model to converge very slow. If your accuracy is increasing but your program finishes before the acuracy reaches a big number, increase your epochs. Lastly, a large batch size might give you a better convergence, but it might also lead to poor generalization and slow training speed. Try batch sizes of 1000, 10000, 200000. We recommend you use a batch sizes of around 150000 for getting your final model, but do experiment with the batch sizes listed above. <u>Q: Do you notice any difference between using batch sizes of 1000, 10000, 200000?</u>\n",
    "    \n",
    "    2. In the function definitions:\n",
    "        * Press the stop button (**interrupt the kernal**) next to Run before you change the values in the functions above. \n",
    "        * Add or reduce Hidden layers if your model turns our poorly. \n",
    "        * Adjust the amount of nodes in each Hidden layer. \n",
    "        * Try out different activation functions. \n",
    "        * Try different regularizers. \n",
    "        * You should aim to get an **mse < 100**.\n",
    "        \n",
    "    3. The program will run for a longer time with large dataset input. Once you get a result with nice mse, you don't have to run `%tensorboard --logdir logs`. Please click on \"File -> Print Preview\" and a separate page should open. Press Cmd/Ctrl + p to print. Select \"Save as PDF\". We will look at your training for the large dataset based on the logs printed out during each epoch.\n",
    "    \n",
    "\n",
    "*Note: Ignore the warnings at the beginning and at the end.*\n",
    "\n",
    "<u>Type your answers to Q:</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "\n",
    "model = modelCreate(LEARNING_RATE)\n",
    "model.fit(train_dataset, batch_size=BATCH_SIZE, epochs=20, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find the accuracy of data is hard to increase, this is because the data in this case is highly inbalance, which, in this case, means that the data cooresponding to no traffic hotspot is far more than the data cooresponding to existing traffic hotspot. You may explore methods of increasing the accuracy by using techniques like increasing sample size, down sampling, up sampling, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Hidden1 (Dense)              (None, 80)                16080     \n",
      "_________________________________________________________________\n",
      "Hidden2 (Dense)              (None, 20)                1620      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 17,742\n",
      "Trainable params: 17,742\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Well Done!\n",
    "**Congradulation on finishing the lab. Submit this .ipnyb Notebook file, the PDF, and loss graph screenshots to the link specified in the Google Doc.**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
